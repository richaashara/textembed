{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TextEmbed \u00b6 TextEmbed is a high-throughput, low-latency REST API for serving vector embeddings. It supports a wide range of sentence-transformer models and frameworks. Developed under the Apache-2.0 License, TextEmbed is designed for flexibility and scalability in embedding applications. Key Features \u00b6 Flexible Model Support : Deploy any model from supported sentence-transformer frameworks, including SentenceTransformers. High-Performance Inference : Utilizes efficient backends like torch, ONNX, TensorRT, and FlashAttention for optimal performance across various devices. Dynamic Batching : Processes new embedding requests as soon as resources are available, ensuring high throughput. Accurate and Tested : Provides embeddings consistent with SentenceTransformers, with unit and end-to-end testing for reliability. User-Friendly API : Built with FastAPI and fully documented via Swagger, aligning with OpenAI\u2019s Embedding specs. Getting Started \u00b6 Installation via PyPI \u00b6 Install TextEmbed: bash pip install -U textembed Start the Server: bash python3 -m textembed.server --models <Model1>,<Model2> --port <Port> View Help Options: bash python3 -m textembed.server --help Running with Docker (Recommended) \u00b6 Pull the Docker Image: bash docker pull kevaldekivadiya/textembed:latest Run the Docker Container: bash docker run -it --gpus all \\ -v $PWD/data:/app/.cache \\ -p 8000:8000 \\ kevaldekivadiya/textembed:latest \\ --models <Model1>,<Model2> \\ --port 8000 View Help Options: bash docker run kevaldekivadiya/textembed:latest --help Accessing the API \u00b6 Access the API documentation via Swagger UI at http://localhost:8000/docs .","title":"Home"},{"location":"#textembed","text":"TextEmbed is a high-throughput, low-latency REST API for serving vector embeddings. It supports a wide range of sentence-transformer models and frameworks. Developed under the Apache-2.0 License, TextEmbed is designed for flexibility and scalability in embedding applications.","title":"TextEmbed"},{"location":"#key-features","text":"Flexible Model Support : Deploy any model from supported sentence-transformer frameworks, including SentenceTransformers. High-Performance Inference : Utilizes efficient backends like torch, ONNX, TensorRT, and FlashAttention for optimal performance across various devices. Dynamic Batching : Processes new embedding requests as soon as resources are available, ensuring high throughput. Accurate and Tested : Provides embeddings consistent with SentenceTransformers, with unit and end-to-end testing for reliability. User-Friendly API : Built with FastAPI and fully documented via Swagger, aligning with OpenAI\u2019s Embedding specs.","title":"Key Features"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#installation-via-pypi","text":"Install TextEmbed: bash pip install -U textembed Start the Server: bash python3 -m textembed.server --models <Model1>,<Model2> --port <Port> View Help Options: bash python3 -m textembed.server --help","title":"Installation via PyPI"},{"location":"#running-with-docker-recommended","text":"Pull the Docker Image: bash docker pull kevaldekivadiya/textembed:latest Run the Docker Container: bash docker run -it --gpus all \\ -v $PWD/data:/app/.cache \\ -p 8000:8000 \\ kevaldekivadiya/textembed:latest \\ --models <Model1>,<Model2> \\ --port 8000 View Help Options: bash docker run kevaldekivadiya/textembed:latest --help","title":"Running with Docker (Recommended)"},{"location":"#accessing-the-api","text":"Access the API documentation via Swagger UI at http://localhost:8000/docs .","title":"Accessing the API"},{"location":"search/","text":"Search Documentation \u00b6 Welcome to the search documentation page for TextEmbed . This page helps you find specific topics and sections within the documentation. Search \u00b6 To search for specific content within this documentation, you can use the following methods: Using the Search Feature in GitHub Pages \u00b6 Open Documentation : Go to the TextEmbed Documentation . Use the Search Bar : Locate the search bar in the top-right corner of the documentation page. It allows you to search through all documentation pages. Manual Search \u00b6 If the search feature is not available or you prefer to manually find information, you can use the following approach: Browse the Table of Contents : Refer to the sidebar or the main page to see the available sections and topics. Use Browser Search : Press Ctrl + F (or Cmd + F on Mac) in your browser to use the built-in search functionality. This will help you find keywords or phrases within the current page. Popular Search Queries \u00b6 Here are some common topics that users often search for: Installation : Details about installing TextEmbed via PyPI or Docker. API Documentation : Access and usage of the TextEmbed API. Key Features : Information about the capabilities and performance of TextEmbed. Getting Started : Initial setup and configuration instructions. If you have specific questions or need further assistance, feel free to contact us or refer to the issue tracker on GitHub. Additional Resources \u00b6 Index Page Setup Guide Swagger Documentation For more detailed information, check out our GitHub Repository and Documentation .","title":"Search"},{"location":"search/#search-documentation","text":"Welcome to the search documentation page for TextEmbed . This page helps you find specific topics and sections within the documentation.","title":"Search Documentation"},{"location":"search/#search","text":"To search for specific content within this documentation, you can use the following methods:","title":"Search"},{"location":"search/#using-the-search-feature-in-github-pages","text":"Open Documentation : Go to the TextEmbed Documentation . Use the Search Bar : Locate the search bar in the top-right corner of the documentation page. It allows you to search through all documentation pages.","title":"Using the Search Feature in GitHub Pages"},{"location":"search/#manual-search","text":"If the search feature is not available or you prefer to manually find information, you can use the following approach: Browse the Table of Contents : Refer to the sidebar or the main page to see the available sections and topics. Use Browser Search : Press Ctrl + F (or Cmd + F on Mac) in your browser to use the built-in search functionality. This will help you find keywords or phrases within the current page.","title":"Manual Search"},{"location":"search/#popular-search-queries","text":"Here are some common topics that users often search for: Installation : Details about installing TextEmbed via PyPI or Docker. API Documentation : Access and usage of the TextEmbed API. Key Features : Information about the capabilities and performance of TextEmbed. Getting Started : Initial setup and configuration instructions. If you have specific questions or need further assistance, feel free to contact us or refer to the issue tracker on GitHub.","title":"Popular Search Queries"},{"location":"search/#additional-resources","text":"Index Page Setup Guide Swagger Documentation For more detailed information, check out our GitHub Repository and Documentation .","title":"Additional Resources"},{"location":"setup/","text":"Setup Guide for TextEmbed \u00b6 This document provides detailed instructions for setting up the TextEmbed server using both PyPI and Docker. Follow the steps below to get started. Prerequisites \u00b6 Ensure you have Python 3.10 or higher installed on your machine. You will also need to install the required dependencies for the TextEmbed server. Installation via PyPI \u00b6 Install the required dependencies: bash pip install -U textembed Start the TextEmbed server with your desired models: bash python3 -m textembed.server --models <Model1>, <Model2> --port <Port> For more information and additional options, run: bash python3 -m textembed.server --help - \u2013models: <Model1> , <Model2> : Comma-separated list of Huggingface models to be used. - \u2013served_model_names: <Name1> , <Name2> : Comma-separated list of names under which the models will be served. - \u2013host: <Host> : The host address on which the application will run. - \u2013port: <Port> : The port number on which the application will run. - \u2013workers: <NumberOfWorkers> : The number of worker processes for batch processing. - \u2013batch_size: <BatchSize> : The batch size for processing requests. - \u2013embedding_dtype: <EmbeddingDtype> : The data type for the embeddings. Choose from \u2018binary\u2019, \u2018float16\u2019, or \u2018float32\u2019. - \u2013api_key: <APIKey> : Your API key for authentication. Make sure to keep it secure. Do not share it with others. Running with Docker (Recommended) \u00b6 You can also run TextEmbed using Docker. The Docker image is available on Docker Hub. Pull the Docker image: bash docker pull kevaldekivadiya/textembed:latest Run the Docker container: bash docker run -p 8000:8000 kevaldekivadiya/textembed:latest --models <Model1>, <Model2> <Host> --port <Port> For more information and additional options, run: bash docker run kevaldekivadiya/textembed:latest --help This command will display the help message for the TextEmbed server, detailing the available options and usage instructions. Accessing the API \u00b6 Once the server is running, you can access the API documentation via Swagger UI by navigating to http://localhost:8000/docs in your web browser. Image Embedding Example \u00b6 With the added support for image models, such as the SentenceTransformer CLIP model ( sentence-transformers/clip-ViT-B-32 ), you can now generate embeddings for images. Steps to Generate Image Embeddings \u00b6 Convert Image to Base64 String: ```python import base64 def image_to_base64(image_path: str) -> str: with open(image_path, \u201crb\u201d) as image_file: encoded_string = base64.b64encode(image_file.read()).decode(\u2018utf-8\u2019) return encoded_string image_path = \u2018 \u2019 base64_string = image_to_base64(image_path) ``` Make a POST Request to the TextEmbed Server: ```python import requests resp = requests.post(url=\u201dhttp://0.0.0.0:8000/v1/image_embedding\u201d, json={ \u201cinput\u201d: [ base64_string ], \u201cmodel\u201d: \u201csentence-transformers/clip-ViT-B-32\u201d, \u201cuser\u201d: \u201cstring\u201d }) print(resp.json()) ``` Example Request and Response \u00b6 Request: { \"input\" : [ \"<Base64EncodedImageString>\" ], \"model\" : \"sentence-transformers/clip-ViT-B-32\" , \"user\" : \"string\" } Response: { \"embeddings\" : [ [ 0.1 , 0.2 , ... , 0.3 ] ], \"model\" : \"sentence-transformers/clip-ViT-B-32\" , \"user\" : \"string\" }","title":"Setup"},{"location":"setup/#setup-guide-for-textembed","text":"This document provides detailed instructions for setting up the TextEmbed server using both PyPI and Docker. Follow the steps below to get started.","title":"Setup Guide for TextEmbed"},{"location":"setup/#prerequisites","text":"Ensure you have Python 3.10 or higher installed on your machine. You will also need to install the required dependencies for the TextEmbed server.","title":"Prerequisites"},{"location":"setup/#installation-via-pypi","text":"Install the required dependencies: bash pip install -U textembed Start the TextEmbed server with your desired models: bash python3 -m textembed.server --models <Model1>, <Model2> --port <Port> For more information and additional options, run: bash python3 -m textembed.server --help - \u2013models: <Model1> , <Model2> : Comma-separated list of Huggingface models to be used. - \u2013served_model_names: <Name1> , <Name2> : Comma-separated list of names under which the models will be served. - \u2013host: <Host> : The host address on which the application will run. - \u2013port: <Port> : The port number on which the application will run. - \u2013workers: <NumberOfWorkers> : The number of worker processes for batch processing. - \u2013batch_size: <BatchSize> : The batch size for processing requests. - \u2013embedding_dtype: <EmbeddingDtype> : The data type for the embeddings. Choose from \u2018binary\u2019, \u2018float16\u2019, or \u2018float32\u2019. - \u2013api_key: <APIKey> : Your API key for authentication. Make sure to keep it secure. Do not share it with others.","title":"Installation via PyPI"},{"location":"setup/#running-with-docker-recommended","text":"You can also run TextEmbed using Docker. The Docker image is available on Docker Hub. Pull the Docker image: bash docker pull kevaldekivadiya/textembed:latest Run the Docker container: bash docker run -p 8000:8000 kevaldekivadiya/textembed:latest --models <Model1>, <Model2> <Host> --port <Port> For more information and additional options, run: bash docker run kevaldekivadiya/textembed:latest --help This command will display the help message for the TextEmbed server, detailing the available options and usage instructions.","title":"Running with Docker (Recommended)"},{"location":"setup/#accessing-the-api","text":"Once the server is running, you can access the API documentation via Swagger UI by navigating to http://localhost:8000/docs in your web browser.","title":"Accessing the API"},{"location":"setup/#image-embedding-example","text":"With the added support for image models, such as the SentenceTransformer CLIP model ( sentence-transformers/clip-ViT-B-32 ), you can now generate embeddings for images.","title":"Image Embedding Example"},{"location":"setup/#steps-to-generate-image-embeddings","text":"Convert Image to Base64 String: ```python import base64 def image_to_base64(image_path: str) -> str: with open(image_path, \u201crb\u201d) as image_file: encoded_string = base64.b64encode(image_file.read()).decode(\u2018utf-8\u2019) return encoded_string image_path = \u2018 \u2019 base64_string = image_to_base64(image_path) ``` Make a POST Request to the TextEmbed Server: ```python import requests resp = requests.post(url=\u201dhttp://0.0.0.0:8000/v1/image_embedding\u201d, json={ \u201cinput\u201d: [ base64_string ], \u201cmodel\u201d: \u201csentence-transformers/clip-ViT-B-32\u201d, \u201cuser\u201d: \u201cstring\u201d }) print(resp.json()) ```","title":"Steps to Generate Image Embeddings"},{"location":"setup/#example-request-and-response","text":"Request: { \"input\" : [ \"<Base64EncodedImageString>\" ], \"model\" : \"sentence-transformers/clip-ViT-B-32\" , \"user\" : \"string\" } Response: { \"embeddings\" : [ [ 0.1 , 0.2 , ... , 0.3 ] ], \"model\" : \"sentence-transformers/clip-ViT-B-32\" , \"user\" : \"string\" }","title":"Example Request and Response"},{"location":"swagger/","text":"Swagger UI Documentation \u00b6 Overview \u00b6 This document provides information about accessing the Swagger UI for the TextEmbed API. Swagger UI is a tool that allows you to interact with and test the API endpoints directly from your browser. Accessing Swagger UI \u00b6 Disclaimer : The Swagger UI you access may differ from the documentation provided in this release, as it is based on the current main branch of the codebase. Swagger UI URL \u00b6 You can view the Swagger UI and interact with the API documentation at: Local Deployment : http://localhost:8000/docs Replace 8000 with the port number you have configured for your TextEmbed server. API Documentation \u00b6 The Swagger UI provides an interactive view of all available API endpoints, including: Endpoints : List of available API endpoints with descriptions. Request Parameters : Details on required and optional parameters for each endpoint. Response Formats : Expected responses and error messages for each API call. Try it Out : Ability to send test requests directly from the Swagger UI. Access Instructions \u00b6 Start the TextEmbed Server : Ensure your TextEmbed server is running locally or use the provided test deployment. Open Swagger UI : Navigate to http://localhost:8000/docs (adjust the port number if needed) in your web browser. Explore the API : Use the Swagger UI to explore and interact with the API endpoints. You can send test requests, view responses, and understand the API\u2019s functionality directly from the UI. Notes \u00b6 The Swagger UI reflects the latest changes from the main branch and may not always match the release version of the API. For questions or issues, refer to the TextEmbed documentation or open an issue on the GitHub repository. Feel free to explore the API and test out various endpoints using the interactive features of Swagger UI.","title":"Swagger"},{"location":"swagger/#swagger-ui-documentation","text":"","title":"Swagger UI Documentation"},{"location":"swagger/#overview","text":"This document provides information about accessing the Swagger UI for the TextEmbed API. Swagger UI is a tool that allows you to interact with and test the API endpoints directly from your browser.","title":"Overview"},{"location":"swagger/#accessing-swagger-ui","text":"Disclaimer : The Swagger UI you access may differ from the documentation provided in this release, as it is based on the current main branch of the codebase.","title":"Accessing Swagger UI"},{"location":"swagger/#swagger-ui-url","text":"You can view the Swagger UI and interact with the API documentation at: Local Deployment : http://localhost:8000/docs Replace 8000 with the port number you have configured for your TextEmbed server.","title":"Swagger UI URL"},{"location":"swagger/#api-documentation","text":"The Swagger UI provides an interactive view of all available API endpoints, including: Endpoints : List of available API endpoints with descriptions. Request Parameters : Details on required and optional parameters for each endpoint. Response Formats : Expected responses and error messages for each API call. Try it Out : Ability to send test requests directly from the Swagger UI.","title":"API Documentation"},{"location":"swagger/#access-instructions","text":"Start the TextEmbed Server : Ensure your TextEmbed server is running locally or use the provided test deployment. Open Swagger UI : Navigate to http://localhost:8000/docs (adjust the port number if needed) in your web browser. Explore the API : Use the Swagger UI to explore and interact with the API endpoints. You can send test requests, view responses, and understand the API\u2019s functionality directly from the UI.","title":"Access Instructions"},{"location":"swagger/#notes","text":"The Swagger UI reflects the latest changes from the main branch and may not always match the release version of the API. For questions or issues, refer to the TextEmbed documentation or open an issue on the GitHub repository. Feel free to explore the API and test out various endpoints using the interactive features of Swagger UI.","title":"Notes"}]}